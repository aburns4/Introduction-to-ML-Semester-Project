{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#importing and parsing the text\n",
    "house_samples = []\n",
    "with open('regressiondata.txt', newline='') as inputfile:\n",
    "    for row in csv.reader(inputfile):\n",
    "        house_samples.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#there are some random first few rows of info regarding the text file that I want to get rid of\n",
    "final_samples = house_samples[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#still parsing the data\n",
    "import numpy as np\n",
    "parsed_samples = np.empty((len(final_samples)-1,5),dtype='object')\n",
    "start_index = 0\n",
    "end_index = 0\n",
    "counter = 0\n",
    "for i in range(0,len(final_samples)):\n",
    "    for j in range(0,len(final_samples[i][0])):\n",
    "        if final_samples[i][0][j] == '\\t':\n",
    "            end_index = j\n",
    "            #print(samples[i][0][start_index:end_index])\n",
    "            parsed_samples[i][counter] = final_samples[i][0][start_index:end_index]\n",
    "            counter+=1\n",
    "            start_index = end_index+1\n",
    "        if counter == 4:\n",
    "            parsed_samples[i][counter] = final_samples[i][0][start_index:len(final_samples[i][0])-1]\n",
    "    counter=0\n",
    "    start_index = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#still parsing the data\n",
    "for i in range(0,len(parsed_samples)):\n",
    "    parsed_samples[i][0] = int(parsed_samples[i][0])\n",
    "    parsed_samples[i][1] = int(parsed_samples[i][1])\n",
    "    parsed_samples[i][3] = int(parsed_samples[i][3])\n",
    "    parsed_samples[i][4] = float(parsed_samples[i][4])\n",
    "    if parsed_samples[i][2] == 'Dublin':\n",
    "        parsed_samples[i][2] = 0\n",
    "    if parsed_samples[i][2] == 'pleasanton':\n",
    "        parsed_samples[i][2] = 1\n",
    "    if parsed_samples[i][2] == 'Clayton':\n",
    "        parsed_samples[i][2] = 2\n",
    "    if parsed_samples[i][2] == 'Moraga':\n",
    "        parsed_samples[i][2] = 3\n",
    "    if parsed_samples[i][2] == 'Antioch':\n",
    "        parsed_samples[i][2] = 4\n",
    "    if parsed_samples[i][2] == 'Danville':\n",
    "        parsed_samples[i][2] = 5\n",
    "    if parsed_samples[i][2] == 'El Dorado Hills':\n",
    "        parsed_samples[i][2] = 6\n",
    "    if parsed_samples[i][2] == 'Shingle Springs':\n",
    "        parsed_samples[i][2] = 7\n",
    "    if parsed_samples[i][2] == 'McKinleyville':\n",
    "        parsed_samples[i][2] = 8\n",
    "    if parsed_samples[i][2] == 'Marina':\n",
    "        parsed_samples[i][2] = 9\n",
    "    if parsed_samples[i][2] == 'Roseville':\n",
    "        parsed_samples[i][2] = 10\n",
    "    if parsed_samples[i][2] == 'Rocklin':\n",
    "        parsed_samples[i][2] = 11\n",
    "    if parsed_samples[i][2] == 'Folsom':\n",
    "        parsed_samples[i][2] = 12\n",
    "    if parsed_samples[i][2] == 'Elk Grove':\n",
    "        parsed_samples[i][2] = 13\n",
    "    if parsed_samples[i][2] == 'Central San Francisco':\n",
    "        parsed_samples[i][2] = 14\n",
    "    if parsed_samples[i][2] == 'Tracy':\n",
    "        parsed_samples[i][2] = 15\n",
    "    if parsed_samples[i][2] == 'Redwood City':\n",
    "        parsed_samples[i][2] = 16\n",
    "    if parsed_samples[i][2] == 'Saratoga':\n",
    "        parsed_samples[i][2] = 17\n",
    "    if parsed_samples[i][2] == 'Morgan Hill':\n",
    "        parsed_samples[i][2] = 18\n",
    "    if parsed_samples[i][2] == 'Mountain View':\n",
    "        parsed_samples[i][2] = 19\n",
    "    if parsed_samples[i][2] == 'Benicia':\n",
    "        parsed_samples[i][2] = 20\n",
    "    if parsed_samples[i][2] == 'Rohnert Park City':\n",
    "        parsed_samples[i][2] = 21\n",
    "    if parsed_samples[i][2] == 'modesto':\n",
    "        parsed_samples[i][2] = 22\n",
    "    if parsed_samples[i][2] == 'Salida':\n",
    "        parsed_samples[i][2] = 23\n",
    "    if parsed_samples[i][2] == 'Oxnard':\n",
    "        parsed_samples[i][2] = 24\n",
    "    if parsed_samples[i][2] == 'Camarillo':\n",
    "        parsed_samples[i][2] = 25\n",
    "    if parsed_samples[i][2] == 'Santa Cruz':\n",
    "        parsed_samples[i][2] = 26\n",
    "    if parsed_samples[i][2] == 'Montecito':\n",
    "        parsed_samples[i][2] = 27\n",
    "    if parsed_samples[i][2] == 'Goleta':\n",
    "        parsed_samples[i][2] = 28\n",
    "    if parsed_samples[i][2] == 'Carpinteria':\n",
    "        parsed_samples[i][2] = 29\n",
    "    if parsed_samples[i][2] == 'San Luis Obispo':\n",
    "        parsed_samples[i][2] = 30\n",
    "    if parsed_samples[i][2] == 'Paso Robles':\n",
    "        parsed_samples[i][2] = 31\n",
    "    if parsed_samples[i][2] == 'Atascadero':\n",
    "        parsed_samples[i][2] = 32\n",
    "    if parsed_samples[i][2] == 'Morro Bay':\n",
    "        parsed_samples[i][2] = 33\n",
    "    if parsed_samples[i][2] == 'Cambria':\n",
    "        parsed_samples[i][2] = 34\n",
    "    if parsed_samples[i][2] == 'La Mirada':\n",
    "        parsed_samples[i][2] = 35\n",
    "    if parsed_samples[i][2] == 'Redondo Beach':\n",
    "        parsed_samples[i][2] = 36\n",
    "    if parsed_samples[i][2] == 'Chino':\n",
    "        parsed_samples[i][2] = 37\n",
    "    if parsed_samples[i][2] == 'Santa Clarita':\n",
    "        parsed_samples[i][2] = 38\n",
    "    if parsed_samples[i][2] == 'Lake Forest':\n",
    "        parsed_samples[i][2] = 39\n",
    "    if parsed_samples[i][2] == 'Seal Beach':\n",
    "        parsed_samples[i][2] = 40\n",
    "    if parsed_samples[i][2] == 'Tustin':\n",
    "        parsed_samples[i][2] = 41\n",
    "    if parsed_samples[i][2] == 'Costa Mesa':\n",
    "        parsed_samples[i][2] = 42\n",
    "    if parsed_samples[i][2] == 'Corona':\n",
    "        parsed_samples[i][2] = 43\n",
    "    if parsed_samples[i][2] == 'Canyon Lake':\n",
    "        parsed_samples[i][2] = 44\n",
    "    if parsed_samples[i][2] == 'Carmel Valley':\n",
    "        parsed_samples[i][2] = 45\n",
    "    if parsed_samples[i][2] == 'Clairmont':\n",
    "        parsed_samples[i][2] = 46\n",
    "    if parsed_samples[i][2] == 'Oceanside':\n",
    "        parsed_samples[i][2] = 47\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn import preprocessing\\nfeatures = preprocessing.normalize(features)\\nlabels = preprocessing.normalize(labels)\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#separating the features and the labels\n",
    "features = np.zeros((len(parsed_samples),4))\n",
    "labels = np.zeros((len(parsed_samples),1))\n",
    "\n",
    "min_ft_0 = 825\n",
    "min_ft_1 = 150000\n",
    "min_ft_2= 0\n",
    "min_ft_3= 1\n",
    "min_ft_4= 1\n",
    "\n",
    "max_ft_0 = 4400\n",
    "max_ft_1 = 1250000\n",
    "max_ft_2 = 47\n",
    "max_ft_3 = 6\n",
    "max_ft_4 = 5\n",
    "\n",
    "for i in range(0,len(parsed_samples)):\n",
    "    features[i][0] = (parsed_samples[i][0]-min_ft_0)/(max_ft_0-min_ft_0)\n",
    "    features[i][1] = (parsed_samples[i][2]-min_ft_2)/(max_ft_2-min_ft_2)\n",
    "    features[i][2] = (parsed_samples[i][3]-min_ft_3)/(max_ft_3-min_ft_3)\n",
    "    features[i][3] = (parsed_samples[i][4]-min_ft_4)/(max_ft_4-min_ft_4)\n",
    "    labels[i] = (parsed_samples[i][1]-min_ft_1)/(max_ft_1-min_ft_1)\n",
    "    \n",
    "'''\n",
    "from sklearn import preprocessing\n",
    "features = preprocessing.normalize(features)\n",
    "labels = preprocessing.normalize(labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#splitting into 70/30 train and test portions\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features,labels, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def gradientDescent(x_train,y_train):\n",
    "    N = len(x_train)\n",
    "    R = .001\n",
    "    w = np.zeros((x_train.shape[1], 1))\n",
    "    total_mn_err = 0\n",
    "    mean_err_update = 0\n",
    "    for z in range(0,N):\n",
    "        mean_err = mean_squared_error(np.dot(w.T,x_train[z]),y_train[z])\n",
    "        total_mn_err += mean_err\n",
    "    total_mn_err = total_mn_err/N\n",
    "    #print(total_mn_err)\n",
    "        \n",
    "    iterations = 200\n",
    "    error = 0\n",
    "    gradient = 0\n",
    "    for k in range(0,iterations):\n",
    "        for i in range(0,N):\n",
    "            error = y_train[i][0] - (np.dot(w.T,x_train[i]))[0]\n",
    "            gradient += error*x_train[i]\n",
    "        #print('before updating, current update value:' + str(R*gradient))\n",
    "        w = w + R*gradient\n",
    "        #print(w)\n",
    "        gradient = 0\n",
    "    return w[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2 = gradientDescent(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.18572897, -0.19536457, -0.10737294, -0.03543807])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_train = np.zeros((len(x_train),1))\n",
    "for l in range(0,len(x_train)):\n",
    "    output_train[l] = np.dot(w2,x_train[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = mean_squared_error(output_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24578655514413425"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_test = np.zeros((len(x_test),1))\n",
    "for l in range(0,len(x_test)):\n",
    "    output_test[l] = np.dot(w2,x_test[l])\n",
    "test_error = mean_squared_error(output_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24897869845934625"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perceptron portion\n",
    "weights = np.zeros((4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .3\n",
    "#let the threshold for the normalized, so let's make labels <.3=-1 and labels >.3=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_train_labels = np.zeros((len(y_train),1))\n",
    "perceptron_test_labels = np.zeros((len(y_test),1))\n",
    "for i in range(0,len(y_train)):\n",
    "    if y_train[i][0] <=.3:\n",
    "        perceptron_train_labels[i] = -1\n",
    "    else:\n",
    "        perceptron_train_labels[i] = 1\n",
    "      \n",
    "for j in range(0,len(y_test)):\n",
    "    if y_test[j][0] <=.3:\n",
    "        perceptron_test_labels[j] = -1\n",
    "    else:\n",
    "        perceptron_test_labels[j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=len(x_train)\n",
    "iterations = 1000\n",
    "R = .001\n",
    "for k in range(0,iterations):\n",
    "    for i in range(0,N):\n",
    "        instance_label = np.sign(weights*x_train[i])\n",
    "        if instance_label[0][0] != np.sign(perceptron_train_labels[i][0]):\n",
    "            weights = weights + R*perceptron_train_labels[i]*x_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_final = weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate train error\n",
    "train_output = np.zeros((len(x_train),1))\n",
    "misclassified = 0\n",
    "for i in range(0,N):\n",
    "    train_output[i] = np.sign(np.dot(weights,x_train[i]))[0]\n",
    "    if train_output[i] != perceptron_train_labels[i][0]:\n",
    "        misclassified +=1\n",
    "\n",
    "misclassified = misclassified/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.93103448275862\n"
     ]
    }
   ],
   "source": [
    "error = misclassified*100\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's see error on test data\n",
    "test_output = np.zeros((len(x_test),1))\n",
    "misclassified_test = 0\n",
    "for i in range(0,len(x_test)):\n",
    "    test_output[i] = np.sign(np.dot(weights,x_test[i]))[0]\n",
    "    if test_output[i] != perceptron_test_labels[i][0]:\n",
    "        misclassified_test +=1\n",
    "\n",
    "misclassified_test = misclassified_test/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.517241379310345\n"
     ]
    }
   ],
   "source": [
    "error_test = misclassified_test*100\n",
    "print(error_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
