{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#Andrea Burns, Machine Learning Project\n",
    "import os, sys, csv\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_frequency_mat(codebook_lengh, wordids, wordcnts):\n",
    "    term_freq = []\n",
    "    for counter, (ids, cnts) in enumerate(zip(wordids, wordcnts)):\n",
    "        vec = np.array([0] * codebook_lengh)\n",
    "        for i, cnt in zip(ids, cnts):\n",
    "            vec[i] = cnt\n",
    "        term_freq.append(vec)\n",
    "        \n",
    "    feature_space = np.vstack(term_freq)\n",
    "    return feature_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_term_frequency():\n",
    "\n",
    "    # ****************************************************************************************************\n",
    "    # load word counts\n",
    "    # ****************************************************************************************************\n",
    "    directory = './'\n",
    "\n",
    "    with open(\"codebook_data.p\", 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "        \n",
    "    code_book = loaded_data[0]\n",
    "    graphlets = loaded_data[1]\n",
    "    codebook_lengh = len(code_book)\n",
    "\n",
    "    uuids, wordids, wordcts = [], [], []\n",
    "    video_uuids, true_labels = [], []\n",
    "    num_of_vids = 493\n",
    "    \n",
    "    for task in range(1, num_of_vids):\n",
    "        file = open(\"vid\" + str(task) + \".p\",'rb')\n",
    "        (uuid, label, ids, histogram) = pickle.load(file, encoding='latin1')\n",
    "        wordids.append(ids)\n",
    "        wordcts.append(histogram)\n",
    "        video_uuids.append(uuid)\n",
    "        true_labels.append(label)\n",
    "    \n",
    "    online_data = wordids, wordcts, true_labels, video_uuids\n",
    "    term_freq = term_frequency_mat(codebook_lengh, wordids, wordcts)\n",
    "    return term_freq, true_labels, code_book, graphlets #online_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_labels = ['microwaving_food','openning_double_doors','printing_interface','printing_take_printout','take_from_fridge','take_paper_towel','take_tea_bag', 'throw_trash','use_kettle','use_water_cooler','washing_up']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def high_instance_code_words(term_frequency, code_book, graphlets, low_instance):\n",
    "    \"\"\"This essentially takes the feature space created over all events, and removes any\n",
    "    feature that is not witnessed in a minimum number of observations (low_instance param).\n",
    "    \"\"\"\n",
    "    ## Number of rows with non zero element :\n",
    "    keep_rows = np.where((term_frequency != 0).sum(axis=0) > low_instance)[0]   # removes code words if they dont appear in a minimum number of videos\n",
    "    # keep_rows = np.where(term_frequency.sum(axis=0) > low_instance)[0]        # removes code words if they dont appear a minimum number of times across all videos\n",
    "\n",
    "    ## Sum of the whole column: term_frequency.sum(axis=0) > low_instance\n",
    "    remove_inds = np.where((term_frequency != 0).sum(axis=0) <= low_instance)[0]\n",
    "\n",
    "    print(\"orig feature space: %s. remove: %s. new space: %s.\" % (len(term_frequency.sum(axis=0)), len(remove_inds), len(keep_rows)))\n",
    "\n",
    "    #keep only the columns of the feature space which have more than low_instance number of occurances.\n",
    "    selected_features = term_frequency.T[keep_rows]\n",
    "    new_term_frequency = selected_features.T\n",
    "    print(\"new feature space shape: \", new_term_frequency.shape)\n",
    "\n",
    "    # # Code Book (1d np array of hash values)\n",
    "    new_code_book = code_book[keep_rows]\n",
    "    print(\"  new code book len: \", len(new_code_book))\n",
    "\n",
    "    # # Graphlets book (1d np array of igraphs)\n",
    "    new_graphlets = graphlets[keep_rows]\n",
    "    print(\"  new graphlet book len: \", len(new_graphlets))\n",
    "\n",
    "    print(\"removed low (%s) instance graphlets\" % low_instance)\n",
    "    print(\"shape = \", new_term_frequency.shape)\n",
    "\n",
    "    return new_term_frequency, new_code_book, new_graphlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(tfreq, tlabels, cbook, glets) = load_term_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig feature space: 22829. remove: 12640. new space: 10189.\n",
      "new feature space shape:  (492, 10189)\n",
      "  new code book len:  10189\n",
      "  new graphlet book len:  10189\n",
      "removed low (1) instance graphlets\n",
      "shape =  (492, 10189)\n"
     ]
    }
   ],
   "source": [
    "(final_tfreq,final_cbook,final_glets) = high_instance_code_words(tfreq,cbook,glets,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['microwaving_food', 'openning_double_doors', 'printing_interface',\n",
       "       'printing_take_printout', 'take_from_fridge', 'take_paper_towel',\n",
       "       'take_tea_bag', 'throw_trash', 'use_kettle', 'use_water_cooler',\n",
       "       'washing_up'], \n",
       "      dtype='<U22')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(tlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = []\n",
    "for i in range(0,len(tlabels)):\n",
    "    if tlabels[i] == 'microwaving_food':\n",
    "        class_labels.append(0)\n",
    "    if tlabels[i] == 'openning_double_doors':\n",
    "        class_labels.append(1)\n",
    "    if tlabels[i] == 'printing_interface':\n",
    "        class_labels.append(2)\n",
    "    if tlabels[i] == 'printing_take_printout':\n",
    "        class_labels.append(3)\n",
    "    if tlabels[i] == 'take_from_fridge':\n",
    "        class_labels.append(4)\n",
    "    if tlabels[i] == 'take_paper_towel':\n",
    "        class_labels.append(5)\n",
    "    if tlabels[i] == 'take_tea_bag':\n",
    "        class_labels.append(6)\n",
    "    if tlabels[i] == 'throw_trash':\n",
    "        class_labels.append(7)\n",
    "    if tlabels[i] == 'use_kettle':\n",
    "        class_labels.append(8)\n",
    "    if tlabels[i] == 'use_water_cooler':\n",
    "        class_labels.append(9)\n",
    "    if tlabels[i] == 'washing_up':\n",
    "        class_labels.append(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_tfreq, class_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.43137255  0.56122449  0.52040816  0.59183673  0.61458333]\n",
      "0.543885054022\n",
      "[ 0.60784314  0.76530612  0.74489796  0.79591837  0.8125    ]\n",
      "0.745293117247\n",
      "[ 0.71568627  0.80612245  0.78571429  0.7755102   0.83333333]\n",
      "0.783273309324\n",
      "[ 0.73529412  0.83673469  0.79591837  0.78571429  0.85416667]\n",
      "0.801565626251\n",
      "[ 0.70588235  0.81632653  0.79591837  0.75510204  0.8125    ]\n",
      "0.777145858343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nI used cross validation to see the best C value, which is .1, as was correctly in the authors code too\\nI just wanted to double check because they don't show how they figure out their C value\\n\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "c_values = [.0001,.001,.01,.1,1]\n",
    "for i in range(0,5):\n",
    "    c = c_values[i]\n",
    "    clf = svm.SVC(kernel='linear', C=c)\n",
    "    scores = cross_val_score(clf, final_tfreq, class_labels, cv=5)\n",
    "    print(scores)\n",
    "    print(sum(scores)/5)\n",
    "'''\n",
    "I used cross validation to see the best C value, which is .1, as was correctly in the authors code too\n",
    "I just wanted to double check because they don't show how they figure out their C value\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.263675470188\n",
      "0.288915566226\n",
      "0.286427070828\n",
      "0.194965486194\n",
      "0.172629051621\n",
      "0.172629051621\n",
      "0.172629051621\n",
      "0.172629051621\n",
      "0.172629051621\n",
      "0.172629051621\n",
      "0.172629051621\n",
      "0.172629051621\n",
      "0.172629051621\n",
      "0.172629051621\n",
      "0.172629051621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nI used cross validation to see the best C value, which is .1, as was correctly in the authors code too\\nI just wanted to double check because they don't show how they figure out their C value\\n\""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammas = [.00001,.0001,.001,.01,.05,.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n",
    "for i in range(0,len(gammas)):\n",
    "    clf_rbf = svm.SVC(kernel='rbf', C=.1,gamma=gammas[i])\n",
    "    scores = cross_val_score(clf_rbf, final_tfreq, class_labels, cv=5)\n",
    "    #print(scores)\n",
    "    print(sum(scores)/5)\n",
    "'''\n",
    "I used cross validation to see the best C value, which is .1, as was correctly in the authors code too\n",
    "I just wanted to double check because they don't show how they figure out their C value\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "classifier = svm.SVC(kernel='linear', C=.1).fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "classifier_nonlinear = svm.SVC(kernel='rbf').fit(X_train, y_train)\n",
    "predicted_nonlinear = classifier_nonlinear.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy for linear kernel: 0.9949109414758269\n",
      "Train accuracy for RBF kernel: 0.9949109414758269\n"
     ]
    }
   ],
   "source": [
    "pred_train = classifier.predict(X_train)\n",
    "pred_train_nonlinear = classifier.predict(X_train)\n",
    "\n",
    "train_accuracy = 0\n",
    "train_accuracy_nonlinear = 0\n",
    "for i in range(0,len(pred_train)):\n",
    "    if pred_train[i] == y_train[i]:\n",
    "        train_accuracy += 1\n",
    "    if pred_train_nonlinear[i] == y_train[i]:\n",
    "        train_accuracy_nonlinear += 1\n",
    "train_accuracy = train_accuracy/len(pred_train)\n",
    "train_accuracy_nonlinear = train_accuracy_nonlinear/len(pred_train_nonlinear)\n",
    "print('Train accuracy for linear kernel: ' + str(train_accuracy))\n",
    "print('Train accuracy for RBF kernel: ' + str(train_accuracy_nonlinear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for linear kernel: 0.797979797979798\n",
      "Test accuracy for RBF kernel: 0.5656565656565656\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "accuracy_nonlinear = 0\n",
    "for i in range(0,len(predicted)):\n",
    "    if predicted[i] == y_test[i]:\n",
    "        accuracy += 1\n",
    "    if predicted_nonlinear[i] == y_test[i]:\n",
    "        accuracy_nonlinear += 1\n",
    "accuracy = accuracy/len(predicted)\n",
    "accuracy_nonlinear = accuracy_nonlinear/len(predicted_nonlinear)\n",
    "print('Test accuracy for linear kernel: ' + str(accuracy))\n",
    "print('Test accuracy for RBF kernel: ' + str(accuracy_nonlinear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf1 = confusion_matrix(y_test, predicted)\n",
    "conf2 = confusion_matrix(y_test, predicted_nonlinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  7,  1,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 11,  0,  0,  1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  6,  0,  3,  1,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  8,  0,  0,  0,  1],\n",
       "       [ 0,  0,  0,  0,  4,  4,  0,  9,  0,  0,  1],\n",
       "       [ 0,  0,  0,  0,  1,  0,  0,  0,  8,  0,  0],\n",
       "       [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0,  0,  0,  0, 17]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2],\n",
       "       [ 0,  0,  3,  3,  0,  0,  0,  0,  0,  0,  2],\n",
       "       [ 0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  2],\n",
       "       [ 0,  0,  0,  0, 11,  0,  0,  0,  1,  0,  0],\n",
       "       [ 0,  0,  0,  0,  6,  0,  0,  2,  0,  0,  2],\n",
       "       [ 0,  0,  0,  0,  0,  0,  8,  0,  0,  0,  1],\n",
       "       [ 0,  0,  0,  0, 12,  0,  0,  4,  0,  0,  2],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  8,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  3],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 18]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V measure score for linear kernel: 0.773796709736\n",
      "Homogeneity score for linear kernel: 0.773916944847\n",
      "Completeness measure score for linear kernel: 0.77367651198\n",
      "Mutual information shared score for linear kernel: 1.73005097406\n",
      "Normalized mutual information shared  score for linear kernel: 0.773796719075\n",
      "\n",
      "V measure score for nonlinear kernel: 0.608339048659\n",
      "Homogeneity score for nonlinear kernel: 0.538942894946\n",
      "Completeness measure score for nonlinear kernel: 0.698247861612\n",
      "Mutual information shared score for nonlinear kernel: 1.20477873831\n",
      "Normalized mutual information shared  score for nonlinear kernel: 0.613445779126\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "vscore = metrics.v_measure_score(y_test, predicted)\n",
    "hscore = metrics.homogeneity_score(y_test, predicted)\n",
    "cscore = metrics.completeness_score(y_test, predicted)\n",
    "mscore = metrics.mutual_info_score(y_test, predicted)\n",
    "nscore = metrics.normalized_mutual_info_score(y_test, predicted)\n",
    "\n",
    "vscore_nonlin = metrics.v_measure_score(y_test, predicted_nonlinear)\n",
    "hscore_nonlin = metrics.homogeneity_score(y_test, predicted_nonlinear)\n",
    "cscore_nonlin = metrics.completeness_score(y_test, predicted_nonlinear)\n",
    "mscore_nonlin = metrics.mutual_info_score(y_test, predicted_nonlinear)\n",
    "nscore_nonlin = metrics.normalized_mutual_info_score(y_test, predicted_nonlinear)\n",
    "\n",
    "print('V measure score for linear kernel: ' + str(vscore))\n",
    "print('Homogeneity score for linear kernel: ' + str(hscore))\n",
    "print('Completeness measure score for linear kernel: ' + str(cscore))\n",
    "print('Mutual information shared score for linear kernel: ' + str(mscore))\n",
    "print('Normalized mutual information shared  score for linear kernel: ' + str(nscore))\n",
    "print()\n",
    "print('V measure score for nonlinear kernel: ' + str(vscore_nonlin))\n",
    "print('Homogeneity score for nonlinear kernel: ' + str(hscore_nonlin))\n",
    "print('Completeness measure score for nonlinear kernel: ' + str(cscore_nonlin))\n",
    "print('Mutual information shared score for nonlinear kernel: ' + str(mscore_nonlin))\n",
    "print('Normalized mutual information shared  score for nonlinear kernel: ' + str(nscore_nonlin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision per class is the diagonal value for that row over the sum of rows\n",
    "precisions = []\n",
    "precisions_nonlinear = []\n",
    "for i in range(0,11):\n",
    "    precisions.append(conf1[i][i]/np.sum(conf1[i][:]))\n",
    "    precisions_nonlinear.append(conf2[i][i]/np.sum(conf2[i][:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision with linear kernel of class microwaving_food is: 1.0\n",
      "Precision with rbf kernel of class microwaving_food is: 0.0\n",
      "\n",
      "Precision with linear kernel of class openning_double_doors is: 1.0\n",
      "Precision with rbf kernel of class openning_double_doors is: 0.0\n",
      "\n",
      "Precision with linear kernel of class printing_interface is: 0.875\n",
      "Precision with rbf kernel of class printing_interface is: 0.375\n",
      "\n",
      "Precision with linear kernel of class printing_take_printout is: 1.0\n",
      "Precision with rbf kernel of class printing_take_printout is: 0.5\n",
      "\n",
      "Precision with linear kernel of class take_from_fridge is: 0.916666666667\n",
      "Precision with rbf kernel of class take_from_fridge is: 0.916666666667\n",
      "\n",
      "Precision with linear kernel of class take_paper_towel is: 0.6\n",
      "Precision with rbf kernel of class take_paper_towel is: 0.0\n",
      "\n",
      "Precision with linear kernel of class take_tea_bag is: 0.888888888889\n",
      "Precision with rbf kernel of class take_tea_bag is: 0.888888888889\n",
      "\n",
      "Precision with linear kernel of class throw_trash is: 0.5\n",
      "Precision with rbf kernel of class throw_trash is: 0.222222222222\n",
      "\n",
      "Precision with linear kernel of class use_kettle is: 0.888888888889\n",
      "Precision with rbf kernel of class use_kettle is: 0.888888888889\n",
      "\n",
      "Precision with linear kernel of class use_water_cooler is: 0.6\n",
      "Precision with rbf kernel of class use_water_cooler is: 0.4\n",
      "\n",
      "Precision with linear kernel of class washing_up is: 0.944444444444\n",
      "Precision with rbf kernel of class washing_up is: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for z in range(0,11):\n",
    "    print('Precision with linear kernel of class ' + text_labels[z] + ' is: ' + str(precisions[z]))\n",
    "    print('Precision with rbf kernel of class ' + text_labels[z] + ' is: ' + str(precisions_nonlinear[z]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall per class is the diagonal value for that row over the sum of columns\n",
    "recalls = []\n",
    "sum_column = 0\n",
    "for i in range(0,11):\n",
    "    for j in range(0,11):\n",
    "        sum_column += conf1[j][i]\n",
    "    recalls.append(conf1[i][i]/sum_column)\n",
    "    sum_column=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls_nonlinear = []\n",
    "sum_column2 = 0\n",
    "for i in range(0,11):\n",
    "    for j in range(0,11):\n",
    "        sum_column2 += conf2[j][i]\n",
    "    if(sum_column2==0):\n",
    "        recalls_nonlinear.append(0)\n",
    "    else:\n",
    "        recalls_nonlinear.append(conf2[i][i]/sum_column2)\n",
    "    sum_column2=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall with linear kernel of class microwaving_food is: 0.666666666667\n",
      "Recall with rbf kernel of class microwaving_food is: 0\n",
      "\n",
      "Recall with linear kernel of class openning_double_doors is: 1.0\n",
      "Recall with rbf kernel of class openning_double_doors is: 0\n",
      "\n",
      "Recall with linear kernel of class printing_interface is: 1.0\n",
      "Recall with rbf kernel of class printing_interface is: 1.0\n",
      "\n",
      "Recall with linear kernel of class printing_take_printout is: 0.8\n",
      "Recall with rbf kernel of class printing_take_printout is: 0.4\n",
      "\n",
      "Recall with linear kernel of class take_from_fridge is: 0.6875\n",
      "Recall with rbf kernel of class take_from_fridge is: 0.379310344828\n",
      "\n",
      "Recall with linear kernel of class take_paper_towel is: 0.545454545455\n",
      "Recall with rbf kernel of class take_paper_towel is: 0\n",
      "\n",
      "Recall with linear kernel of class take_tea_bag is: 1.0\n",
      "Recall with rbf kernel of class take_tea_bag is: 1.0\n",
      "\n",
      "Recall with linear kernel of class throw_trash is: 0.692307692308\n",
      "Recall with rbf kernel of class throw_trash is: 0.666666666667\n",
      "\n",
      "Recall with linear kernel of class use_kettle is: 0.888888888889\n",
      "Recall with rbf kernel of class use_kettle is: 0.888888888889\n",
      "\n",
      "Recall with linear kernel of class use_water_cooler is: 1.0\n",
      "Recall with rbf kernel of class use_water_cooler is: 0.4\n",
      "\n",
      "Recall with linear kernel of class washing_up is: 0.894736842105\n",
      "Recall with rbf kernel of class washing_up is: 0.529411764706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for z in range(0,11):\n",
    "    print('Recall with linear kernel of class ' + text_labels[z] + ' is: ' + str(recalls[z]))\n",
    "    print('Recall with rbf kernel of class ' + text_labels[z] + ' is: ' + str(recalls_nonlinear[z]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 value with nonlinear kernel of class microwaving_food is: 0.8\n",
      "F1 value with nonlinear kernel of class microwaving_food is: 0\n",
      "\n",
      "F1 value with nonlinear kernel of class openning_double_doors is: 1.0\n",
      "F1 value with nonlinear kernel of class openning_double_doors is: 0\n",
      "\n",
      "F1 value with nonlinear kernel of class printing_interface is: 0.933333333333\n",
      "F1 value with nonlinear kernel of class printing_interface is: 0.545454545455\n",
      "\n",
      "F1 value with nonlinear kernel of class printing_take_printout is: 0.888888888889\n",
      "F1 value with nonlinear kernel of class printing_take_printout is: 0.444444444444\n",
      "\n",
      "F1 value with nonlinear kernel of class take_from_fridge is: 0.785714285714\n",
      "F1 value with nonlinear kernel of class take_from_fridge is: 0.536585365854\n",
      "\n",
      "F1 value with nonlinear kernel of class take_paper_towel is: 0.571428571429\n",
      "F1 value with nonlinear kernel of class take_paper_towel is: 0\n",
      "\n",
      "F1 value with nonlinear kernel of class take_tea_bag is: 0.941176470588\n",
      "F1 value with nonlinear kernel of class take_tea_bag is: 0.941176470588\n",
      "\n",
      "F1 value with nonlinear kernel of class throw_trash is: 0.58064516129\n",
      "F1 value with nonlinear kernel of class throw_trash is: 0.333333333333\n",
      "\n",
      "F1 value with nonlinear kernel of class use_kettle is: 0.888888888889\n",
      "F1 value with nonlinear kernel of class use_kettle is: 0.888888888889\n",
      "\n",
      "F1 value with nonlinear kernel of class use_water_cooler is: 0.75\n",
      "F1 value with nonlinear kernel of class use_water_cooler is: 0.4\n",
      "\n",
      "F1 value with nonlinear kernel of class washing_up is: 0.918918918919\n",
      "F1 value with nonlinear kernel of class washing_up is: 0.692307692308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_values = []\n",
    "f_values_nonlin = []\n",
    "for i in range(0,11):\n",
    "    f_values.append(2*precisions[i]*recalls[i]/(precisions[i]+recalls[i]))\n",
    "    print('F1 value with nonlinear kernel of class ' + text_labels[i] + ' is: ' + str(f_values[i]))\n",
    "    if(precisions_nonlinear[i]+recalls_nonlinear[i]==0):\n",
    "            f_values_nonlin.append(0)\n",
    "    else:\n",
    "        f_values_nonlin.append(2*precisions_nonlinear[i]*recalls_nonlinear[i]/(precisions_nonlinear[i]+recalls_nonlinear[i]))\n",
    "    print('F1 value with nonlinear kernel of class ' + text_labels[i] + ' is: ' + str(f_values_nonlin[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39863452183\n",
      "0.347435298619\n",
      "0.46895771842\n",
      "0.780699656933\n",
      "0.403381017711\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Wanted to attempt unsupervised clustering, similar to what others did. For some reason they used 10 clusters,\n",
    "but I think 11 make more sense because there are 11 activity classes. I'll try it with both 10 and 11. I also \n",
    "now rerun the clustering and average the metrics over 10 runs so that they are more telling of the clustering,\n",
    "just as the authors did.\n",
    "'''\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "vscores = []\n",
    "hscores = []\n",
    "cscores = []\n",
    "mscores = []\n",
    "nscores = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    kmeans = KMeans(n_clusters=11,init='k-means++')\n",
    "    kmeans.fit(final_tfreq)\n",
    "\n",
    "    pred_labels = kmeans.labels_\n",
    "    \n",
    "    vscore = metrics.v_measure_score(class_labels, pred_labels)\n",
    "    hscore = metrics.homogeneity_score(class_labels, pred_labels)\n",
    "    cscore = metrics.completeness_score(class_labels, pred_labels)\n",
    "    mscore = metrics.mutual_info_score(class_labels, pred_labels)\n",
    "    nscore = metrics.normalized_mutual_info_score(class_labels, pred_labels)\n",
    "    \n",
    "    vscores.append(vscore)\n",
    "    hscores.append(hscore)\n",
    "    cscores.append(cscore)\n",
    "    mscores.append(mscore)\n",
    "    nscores.append(nscore)\n",
    "    \n",
    "'''\n",
    "these are five different evaluation metrics typically used for k-means or unsupervised learning paradigms\n",
    "v_measure score is supposed to be the same as the normalized mutual info score but they're slightly different-\n",
    "not sure why. v measure/normalized mutual info score is the measure of similarity between the two labels of the same\n",
    "data, but normalized to be between 0 and 1, with 1 being a perfect correlation adn 0 being no mutual information.\n",
    "mutual_info_score is just not normalized yet.\n",
    "\n",
    "Homogeneity: if all of its clusters contain only data points which are members of a single class\n",
    "Completeness: if all the data points that are members of a given class are elements of the same cluster\n",
    "'''\n",
    "overall_v = sum(vscores)/len(vscores)\n",
    "overall_h = sum(hscores)/len(hscores)\n",
    "overall_c = sum(cscores)/len(cscores)\n",
    "overall_m = sum(mscores)/len(mscores)\n",
    "overall_n = sum(nscores)/len(nscores)\n",
    "\n",
    "print(overall_v)\n",
    "print(overall_h)\n",
    "print(overall_c)\n",
    "print(overall_m)\n",
    "print(overall_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeanShift(bandwidth=None, bin_seeding=False, cluster_all=True, min_bin_freq=1,\n",
       "     n_jobs=1, seeds=None)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "I'm now going to try MeanShift clustering because it doesn't necessarily assign all data points to a cluster,\n",
    "and doesn't ask for a specific number of clusters as input. I think this data is probably pretty noisy \n",
    "based off the results with kmeans, so I'd like to see how different the results end up being\n",
    "'''\n",
    "from sklearn.cluster import MeanShift\n",
    "mean_cluster = MeanShift()\n",
    "mean_cluster.fit(final_tfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mc_labels = mean_cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22257772949\n",
      "0.171868799082\n",
      "0.31573298559\n",
      "0.386195395268\n",
      "0.232947738911\n"
     ]
    }
   ],
   "source": [
    "mc_vscore = metrics.v_measure_score(class_labels, mc_labels)\n",
    "mc_hscore = metrics.homogeneity_score(class_labels, mc_labels)\n",
    "mc_cscore = metrics.completeness_score(class_labels, mc_labels)\n",
    "mc_mscore = metrics.mutual_info_score(class_labels, mc_labels)\n",
    "mc_nscore = metrics.normalized_mutual_info_score(class_labels, mc_labels)\n",
    "\n",
    "print(mc_vscore)\n",
    "print(mc_hscore)\n",
    "print(mc_cscore)\n",
    "print(mc_mscore)\n",
    "print(mc_nscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48200689336\n",
      "0.583819239325\n",
      "0.410431642691\n",
      "1.31186290415\n",
      "0.489507803238\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "It appears that meanshift actually performs significantly worse because it just doens't account for\n",
    "a lot of the noise, and a large amount of the data is noisy so this actually doesn't help. I'll try affinity \n",
    "propagation clustering below which does account for the noise and assigns each data point to a cluster.\n",
    "'''\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "af_cluster = AffinityPropagation()\n",
    "af_cluster.fit(final_tfreq)\n",
    "af_labels = af_cluster.labels_\n",
    "\n",
    "af_vscore = metrics.v_measure_score(class_labels, af_labels)\n",
    "af_hscore = metrics.homogeneity_score(class_labels, af_labels)\n",
    "af_cscore = metrics.completeness_score(class_labels, af_labels)\n",
    "af_mscore = metrics.mutual_info_score(class_labels, af_labels)\n",
    "af_nscore = metrics.normalized_mutual_info_score(class_labels, af_labels)\n",
    "\n",
    "print(af_vscore)\n",
    "print(af_hscore)\n",
    "print(af_cscore)\n",
    "print(af_mscore)\n",
    "print(af_nscore)\n",
    "#seems to do a much better job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.394215876887\n",
      "0.339945494957\n",
      "0.46910600636\n",
      "0.763869797752\n",
      "0.399337543338\n"
     ]
    }
   ],
   "source": [
    "#trying with 10 clusters now\n",
    "vscores_ten = []\n",
    "hscores_ten = []\n",
    "cscores_ten = []\n",
    "mscores_ten = []\n",
    "nscores_ten = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    kmeans = KMeans(n_clusters=10,init='k-means++')\n",
    "    kmeans.fit(final_tfreq)\n",
    "\n",
    "    pred_labels = kmeans.labels_\n",
    "    \n",
    "    vscore_ten = metrics.v_measure_score(class_labels, pred_labels_ten)\n",
    "    hscore_ten = metrics.homogeneity_score(class_labels, pred_labels_ten)\n",
    "    cscore_ten = metrics.completeness_score(class_labels, pred_labels_ten)\n",
    "    mscore_ten = metrics.mutual_info_score(class_labels, pred_labels_ten)\n",
    "    nscore_ten = metrics.normalized_mutual_info_score(class_labels, pred_labels_ten)\n",
    "    \n",
    "    vscores_ten.append(vscore_ten)\n",
    "    hscores_ten.append(hscore_ten)\n",
    "    cscores_ten.append(cscore_ten)\n",
    "    mscores_ten.append(mscore_ten)\n",
    "    nscores_ten.append(nscore_ten)\n",
    "    \n",
    "overall_v_ten = sum(vscores_ten)/len(vscores_ten)\n",
    "overall_h_ten = sum(hscores_ten)/len(hscores_ten)\n",
    "overall_c_ten = sum(cscores_ten)/len(cscores_ten)\n",
    "overall_m_ten = sum(mscores_ten)/len(mscores_ten)\n",
    "overall_n_ten = sum(nscores_ten)/len(nscores_ten)\n",
    "\n",
    "print(overall_v_ten)\n",
    "print(overall_h_ten)\n",
    "print(overall_c_ten)\n",
    "print(overall_m_ten)\n",
    "print(overall_n_ten)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
