{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#Andrea Burns, Project train and test\n",
    "import os, sys, csv\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_frequency_mat(codebook_lengh, wordids, wordcnts):\n",
    "    term_freq = []\n",
    "    for counter, (ids, cnts) in enumerate(zip(wordids, wordcnts)):\n",
    "        vec = np.array([0] * codebook_lengh)\n",
    "        for i, cnt in zip(ids, cnts):\n",
    "            vec[i] = cnt\n",
    "        term_freq.append(vec)\n",
    "        \n",
    "    feature_space = np.vstack(term_freq)\n",
    "    return feature_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_term_frequency():\n",
    "\n",
    "    # ****************************************************************************************************\n",
    "    # load word counts\n",
    "    # ****************************************************************************************************\n",
    "    directory = './'\n",
    "\n",
    "    with open(\"codebook_data.p\", 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "        \n",
    "    code_book = loaded_data[0]\n",
    "    graphlets = loaded_data[1]\n",
    "    codebook_lengh = len(code_book)\n",
    "\n",
    "    uuids, wordids, wordcts = [], [], []\n",
    "    video_uuids, true_labels = [], []\n",
    "    num_of_vids = 493\n",
    "    \n",
    "    for task in range(1, num_of_vids):\n",
    "        file = open(\"vid\" + str(task) + \".p\",'rb')\n",
    "        (uuid, label, ids, histogram) = pickle.load(file, encoding='latin1')\n",
    "        wordids.append(ids)\n",
    "        wordcts.append(histogram)\n",
    "        video_uuids.append(uuid)\n",
    "        true_labels.append(label)\n",
    "    \n",
    "    online_data = wordids, wordcts, true_labels, video_uuids\n",
    "    term_freq = term_frequency_mat(codebook_lengh, wordids, wordcts)\n",
    "    return term_freq, true_labels#online_data, code_book, graphlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory:  ./\n"
     ]
    }
   ],
   "source": [
    "(a,b) = load_term_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['microwaving_food' 'openning_double_doors' 'printing_interface'\n",
      " 'printing_take_printout' 'take_from_fridge' 'take_paper_towel'\n",
      " 'take_tea_bag' 'throw_trash' 'use_kettle' 'use_water_cooler' 'washing_up']\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = []\n",
    "for i in range(0,len(b)):\n",
    "    if b[i] == 'microwaving_food':\n",
    "        class_labels.append(0)\n",
    "    if b[i] == 'openning_double_doors':\n",
    "        class_labels.append(1)\n",
    "    if b[i] == 'printing_interface':\n",
    "        class_labels.append(2)\n",
    "    if b[i] == 'printing_take_printout':\n",
    "        class_labels.append(3)\n",
    "    if b[i] == 'take_from_fridge':\n",
    "        class_labels.append(4)\n",
    "    if b[i] == 'take_paper_towel':\n",
    "        class_labels.append(5)\n",
    "    if b[i] == 'take_tea_bag':\n",
    "        class_labels.append(6)\n",
    "    if b[i] == 'throw_trash':\n",
    "        class_labels.append(7)\n",
    "    if b[i] == 'use_kettle':\n",
    "        class_labels.append(8)\n",
    "    if b[i] == 'use_water_cooler':\n",
    "        class_labels.append(9)\n",
    "    if b[i] == 'washing_up':\n",
    "        class_labels.append(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def high_instance_code_words(term_frequency, code_book, graphlets, low_instance):\n",
    "    \"\"\"This essentially takes the feature space created over all events, and removes any\n",
    "    feature that is not witnessed in a minimum number of observations (low_instance param).\n",
    "    \"\"\"\n",
    "    ## Number of rows with non zero element :\n",
    "    keep_rows = np.where((term_frequency != 0).sum(axis=0) > low_instance)[0]   # removes code words if they dont appear in a minimum number of videos\n",
    "    # keep_rows = np.where(term_frequency.sum(axis=0) > low_instance)[0]        # removes code words if they dont appear a minimum number of times across all videos\n",
    "\n",
    "    ## Sum of the whole column: term_frequency.sum(axis=0) > low_instance\n",
    "    remove_inds = np.where((term_frequency != 0).sum(axis=0) <= low_instance)[0]\n",
    "\n",
    "    print(\"orig feature space: %s. remove: %s. new space: %s.\" % (len(term_frequency.sum(axis=0)), len(remove_inds), len(keep_rows)))\n",
    "\n",
    "    #keep only the columns of the feature space which have more than low_instance number of occurances.\n",
    "    selected_features = term_frequency.T[keep_rows]\n",
    "    new_term_frequency = selected_features.T\n",
    "    print(\"new feature space shape: \", new_term_frequency.shape)\n",
    "\n",
    "    # # Code Book (1d np array of hash values)\n",
    "    new_code_book = code_book[keep_rows]\n",
    "    print(\"  new code book len: \", len(new_code_book))\n",
    "\n",
    "    # # Graphlets book (1d np array of igraphs)\n",
    "    new_graphlets = graphlets[keep_rows]\n",
    "    print(\"  new graphlet book len: \", len(new_graphlets))\n",
    "\n",
    "    print(\"removed low (%s) instance graphlets\" % low_instance)\n",
    "    print(\"shape = \", new_term_frequency.shape)\n",
    "\n",
    "    return new_term_frequency, new_code_book, new_graphlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory:  ./\n"
     ]
    }
   ],
   "source": [
    "(a,b,c,d) = load_term_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig feature space: 22829. remove: 12640. new space: 10189.\n",
      "new feature space shape:  (492, 10189)\n",
      "  new code book len:  10189\n",
      "  new graphlet book len:  10189\n",
      "removed low (1) instance graphlets\n",
      "shape =  (492, 10189)\n"
     ]
    }
   ],
   "source": [
    "(e,f,g) = high_instance_code_words(a,c,d,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(e, class_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "classifier = svm.SVC(kernel='linear', C=.1).fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.797979797979798\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for i in range(0,len(predicted)):\n",
    "    if predicted[i] == y_test[i]:\n",
    "        accuracy += 1\n",
    "accuracy = accuracy/len(predicted)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
